#!/bin/bash
#SBATCH -p gpu_titanrtx
#SBATCH -t 96:00:00
#SBATCH -N 1 
#SBATCH -n 4
source /home/davidr/scripts/nki_tf2.sh

JOBSPERNODE=4

# Send node names to array.
IFS=',' read -ra NODE_ARRAY <<< "$SLURM_JOB_NODELIST"
HOSTS=""
for NODE in "${NODE_ARRAY[@]}"; do
    echo $NODE; 
    HOSTS="${HOSTS}$NODE:$JOBSPERNODE,"; 
done
HOSTS=${HOSTS::-1}

echo ${HOSTS}  

horovodrun -np $SLURM_NTASKS -H ${HOSTS} --start-timeout 120 --verbose python train.py /project/davidr/lidc_idri/ 512 --base_dim 256 --latent_dim 256 --mixing_epochs 256 --stabilizing_epochs 256 --training_ratio 1 --starting_phase 1 --phase_1_batch_size 64 --horovod
